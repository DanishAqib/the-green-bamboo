{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapper 1: Scrape Search Results\n",
    "Scrape from `https://untappd.com/search?q={country}&type=brewery&sort=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMPORT PACKAGES\n",
    "# # ============\n",
    "# # === BEAUTIFULSOUP ===\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # === DOTENV ===\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # === OS ===\n",
    "# import os\n",
    "\n",
    "# # === PANDAS ===\n",
    "# import pandas as pd\n",
    "\n",
    "# # === PICKLE ===\n",
    "# import pickle\n",
    "\n",
    "# # === SELENIUM ===\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# # === TIME ===\n",
    "# import time\n",
    "\n",
    "# # =============================================================================\n",
    "\n",
    "# # SAVE LOGIN DETAILS\n",
    "# # ============\n",
    "\n",
    "# COOKIES_FILE_PATH = 'cookies.pkl'\n",
    "\n",
    "# def save_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "#     with open(path, 'wb') as file:\n",
    "#         pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "# def load_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "#     with open(path, 'rb') as file:\n",
    "#         cookies = pickle.load(file)\n",
    "#         for cookie in cookies:\n",
    "#             driver.add_cookie(cookie)\n",
    "\n",
    "# # =============================================================================\n",
    "\n",
    "# # Constants\n",
    "# BASE_URL = \"https://untappd.com\"\n",
    "# LOGIN_URL = f\"{BASE_URL}/login\"\n",
    "# HOME_URL = f\"{BASE_URL}/home\"\n",
    "\n",
    "# # Credentials\n",
    "# load_dotenv()\n",
    "# USERNAME = os.getenv('USERNAME')\n",
    "# PASSWORD = os.getenv('PASSWORD')\n",
    "\n",
    "# # Countries\n",
    "# COUNTRIES = [\n",
    "#     # \"singapore\",\n",
    "#     # \"laos\",\n",
    "#     # \"malaysia\",\n",
    "#     # \"cambodia\",\n",
    "#     # \"indonesia\",\n",
    "#     # \"hong kong\",\n",
    "#     # \"philippines\",\n",
    "#     # \"taiwan\",\n",
    "#     # \"vietnam\",\n",
    "#     # \"israel\",\n",
    "#     # \"india\",\n",
    "#     # \"thailand\",\n",
    "#     # \"korea\",\n",
    "#     # \"japan\",\n",
    "#     # \"china\",\n",
    "#     # \"new zealand\",\n",
    "#     # \"australia\"\n",
    "# ]\n",
    "\n",
    "# # Define the directory containing the CSV files\n",
    "# DIRECTORY = \"../Results/Untappd Search\"\n",
    "\n",
    "# def last_csv(countries):\n",
    "#     csv_files = []\n",
    "    \n",
    "#     for country in countries:\n",
    "#         # Construct the full file name with .csv extension\n",
    "#         file_name = f\"{country}.csv\"\n",
    "#         file_path = os.path.join(DIRECTORY, file_name)\n",
    "        \n",
    "#         # Check if the file exists in the directory\n",
    "#         if os.path.exists(file_path):\n",
    "#             csv_files.append(file_path)  # Store the full path instead of just the file name\n",
    "    \n",
    "#     # Return the path of the last CSV file found, or None if no files were found\n",
    "#     return csv_files[-1] if csv_files else None\n",
    "\n",
    "# def last_record(file_path):\n",
    "#     if file_path is not None:\n",
    "#         try:\n",
    "#             # Load the CSV file into a DataFrame\n",
    "#             df = pd.read_csv(file_path)\n",
    "            \n",
    "#             # Check if the DataFrame is not empty\n",
    "#             if not df.empty:\n",
    "#                 # Return the last row of the DataFrame\n",
    "#                 return df.iloc[-1]\n",
    "#             else:\n",
    "#                 return None\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading {file_path}: {e}\")\n",
    "#             return None\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Initialize Selenium WebDriver\n",
    "# driver = webdriver.Chrome()\n",
    "# driver.maximize_window()\n",
    "\n",
    "# def login():\n",
    "#     # MANUAL STEPS\n",
    "#     # ============\n",
    "#     # 1. Verify the CAPTCHA\n",
    "#     # 2. Press the Enter key to continue after verifying the CAPTCHA\n",
    "#     # 3. Press \"Enter\" key in VSCode terminal to continue (ensures that login has been successful)\n",
    "\n",
    "#     driver.get(LOGIN_URL)\n",
    "#     wait = WebDriverWait(driver, 10)\n",
    "\n",
    "#     # Load cookies if they exist and check if already logged in\n",
    "#     if os.path.exists(COOKIES_FILE_PATH):\n",
    "#         load_cookies(driver)\n",
    "#         driver.get(HOME_URL)  # Navigate to home to check if login was successful\n",
    "#         if driver.current_url == HOME_URL:\n",
    "#             print(\"Logged in using saved cookies\")\n",
    "#             return\n",
    "#         else:\n",
    "#             print(\"Failed to log in with saved cookies. Proceeding with manual login.\")\n",
    "\n",
    "#     # Enter username\n",
    "#     username_input = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "#     username_input.send_keys(USERNAME)\n",
    "\n",
    "#     # Enter password\n",
    "#     password_input = driver.find_element(By.ID, 'password')\n",
    "#     password_input.send_keys(PASSWORD)\n",
    "\n",
    "#     try:\n",
    "#         # Wait for the CAPTCHA and solve it manually if it appears\n",
    "#         captcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "#         if captcha_frame:\n",
    "#             print(\"Please solve the CAPTCHA manually if it appears, then press Enter to continue...\")\n",
    "#             input()\n",
    "#     except:\n",
    "#         # No CAPTCHA present, continue with login\n",
    "#         pass\n",
    "\n",
    "#     # Wait until redirected to HOME_URL\n",
    "#     wait.until(EC.url_to_be(HOME_URL))\n",
    "#     print(\"Login successful, redirected to home\")\n",
    "\n",
    "#     # Save cookies after successful login\n",
    "#     save_cookies(driver)\n",
    "\n",
    "# def get_soup():\n",
    "#     time.sleep(2)  # Ensure the page has fully loaded\n",
    "#     return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# def click_show_more(class_name):\n",
    "#     time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "#     # Check if there is an \"announcement\" modal\n",
    "#     try:\n",
    "#         announcement_modal = driver.find_element(By.CLASS_NAME, 'announcement')\n",
    "#         if announcement_modal:\n",
    "#             track_button = announcement_modal.find_element(By.CLASS_NAME, 'track-click')\n",
    "#             track_button.click()\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     while True:\n",
    "#         try:\n",
    "#             # Scroll to the bottom of the page\n",
    "#             driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#             time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "#             # Find and click the \"Show More\" button\n",
    "#             show_more_button = WebDriverWait(driver, 10).until(\n",
    "#                 EC.element_to_be_clickable((By.CLASS_NAME, class_name))\n",
    "#             )\n",
    "#             ActionChains(driver).move_to_element(show_more_button).click().perform()\n",
    "#             time.sleep(2)  # Ensure the new content has loaded\n",
    "#         except Exception as e:\n",
    "#             break\n",
    "\n",
    "# def parse_breweries(soup):\n",
    "#     breweries = []\n",
    "#     for brewery in soup.select('div.beer-details'):\n",
    "#         name = brewery.select_one('p.name a').text.strip()\n",
    "#         link = BASE_URL + brewery.select_one('p.name a')['href']\n",
    "#         breweries.append((name, link))\n",
    "#     print(f\"Found {len(breweries)} breweries\")\n",
    "#     return breweries\n",
    "\n",
    "# def parse_breweries_details(soup):\n",
    "#     country = soup.select_one('p.brewery').text.strip() if soup.select_one('p.brewery') else \"\"\n",
    "#     return country\n",
    "\n",
    "# def parse_beers(soup):\n",
    "#     beers = []\n",
    "#     for beer in soup.select('div.beer-details'):\n",
    "#         name = beer.select_one('p.name a').text.strip()\n",
    "#         link = BASE_URL + beer.select_one('p.name a')['href']\n",
    "#         beers.append((name, link))\n",
    "#     print(f\"Found {len(beers)} beers\")\n",
    "#     return beers\n",
    "\n",
    "# def parse_beer_details(soup):\n",
    "#     abv = soup.select_one('p.abv').text.strip() if soup.select_one('p.abv') else \"\"\n",
    "#     drink_category = soup.select_one('div.name p.style').text.strip() if soup.select_one('div.name p.style') else \"\"\n",
    "#     latest_review_date = soup.select_one('div.bottom a.time')['data-gregtime'] if soup.select_one('div.bottom a.time') else \"\"\n",
    "#     official_description = soup.select_one('div.beer-descrption-read-less').text.strip() if soup.select_one('div.beer-descrption-read-less') else \"\"\n",
    "#     if official_description.endswith(\" Show Less\"):\n",
    "#         official_description = official_description[:-10]  # Remove the last 10 characters (\" Show Less\")\n",
    "#     photo_link = soup.select_one('a.label.image-big')['data-image'] if soup.select_one('a.label.image-big') else \"\"\n",
    "#     return abv, drink_category, latest_review_date, official_description, photo_link\n",
    "\n",
    "# def scrape_untappd():\n",
    "#     login()  # Perform login\n",
    "\n",
    "#     # Extract last scraped details\n",
    "#     last_csv_file = last_csv(COUNTRIES)  # Get the last CSV file path\n",
    "#     last_row = last_record(last_csv_file)  # Get the last row from the last CSV file\n",
    "\n",
    "#     start_brewery_idx = 0\n",
    "#     start_beer_idx = 0\n",
    "#     start_country_idx = 0\n",
    "\n",
    "#     if last_csv_file is not None and last_row is not None:\n",
    "#         # Continue from last scraped brewery and beer\n",
    "#         start_brewery_idx = last_row['Brewery Index'] - 1\n",
    "#         start_beer_idx = last_row['Beer Index']\n",
    "#         # Find the index of the country to start from\n",
    "#         for idx, country in enumerate(COUNTRIES):\n",
    "#             if country in last_csv_file:\n",
    "#                 start_country_idx = idx\n",
    "#                 break\n",
    "\n",
    "#     for country_idx, search_country in enumerate(COUNTRIES[start_country_idx:], start=start_country_idx):\n",
    "#         # Check if \"search_country\" contains \" \", replace with \"+\" if it does\n",
    "#         search_country_formatted = search_country\n",
    "#         if \" \" in search_country:\n",
    "#             search_country_formatted = search_country.replace(\" \", \"+\")\n",
    "#         print(\"----------------------------------------\")\n",
    "#         print(f\"Scraping country: {search_country}\")\n",
    "\n",
    "#         SEARCH_URL = f\"{BASE_URL}/search?q={search_country_formatted}&type=brewery&sort=\"\n",
    "#         driver.get(SEARCH_URL)\n",
    "#         click_show_more('more_search')  # Ensure all breweries are loaded\n",
    "#         soup = get_soup()\n",
    "#         breweries = parse_breweries(soup)\n",
    "\n",
    "#         beer_data = []\n",
    "\n",
    "#         for brewery_idx, (brewery_name, brewery_url) in enumerate(breweries):\n",
    "#             if country_idx == start_country_idx and brewery_idx < start_brewery_idx:\n",
    "#                 continue\n",
    "\n",
    "#             print(\"==================================================\")\n",
    "#             print(f\"Scraping brewery {brewery_idx + 1}/{len(breweries)}: {brewery_name}\")\n",
    "\n",
    "#             # Step B: Scrape beers from each brewery\n",
    "#             driver.get(f\"{brewery_url}/beer\")\n",
    "#             click_show_more('more-list-items')  # Ensure all beers are loaded\n",
    "#             soup = get_soup()\n",
    "#             beers = parse_beers(soup)\n",
    "\n",
    "#             # Step C: Scrape brewery details\n",
    "#             country = parse_breweries_details(soup)\n",
    "#             for beer_idx, (beer_name, beer_url) in enumerate(beers):\n",
    "#                 if country_idx == start_country_idx and brewery_idx == start_brewery_idx and beer_idx < start_beer_idx:\n",
    "#                     continue\n",
    "\n",
    "#                 print(f\"Scraping beer {beer_idx + 1}/{len(beers)}: {beer_name}\")\n",
    "\n",
    "#                 # Step D: Scrape beer details\n",
    "#                 driver.get(beer_url)\n",
    "#                 beer_soup = get_soup()\n",
    "#                 abv, drink_category, latest_review_date, official_description, photo_link = parse_beer_details(beer_soup)\n",
    "\n",
    "#                 # Collect beer data\n",
    "#                 beer_data = {\n",
    "#                     'Country': country,\n",
    "#                     'Brewery Index': brewery_idx + 1,\n",
    "#                     'Beer Index': beer_idx + 1,\n",
    "#                     'Scraping Remarks / Errors': '',  # Add any remarks if needed\n",
    "#                     'Date of latest review': latest_review_date,\n",
    "#                     'Expression Name': beer_name,\n",
    "#                     'Producer': brewery_name,\n",
    "#                     'Bottler': '', # Leave blank for now\n",
    "#                     'Country of Origin': country,\n",
    "#                     'Drink Type': 'Beer',\n",
    "#                     'Drink Category': drink_category,\n",
    "#                     'Age': '',\n",
    "#                     'ABV': abv,\n",
    "#                     '88B Website Review Link': '', # Leave blank for now\n",
    "#                     'Official Description': official_description,\n",
    "#                     'Producer Website Link': '',\n",
    "#                     'Photo Link': photo_link\n",
    "#                 }\n",
    "\n",
    "#                 # Step E: Save the collected data to CSV\n",
    "#                 # Save for each beer scraped\n",
    "\n",
    "#                 csv_file_path = f'{DIRECTORY}/{search_country}.csv'\n",
    "\n",
    "#                 # Check if the CSV file exists\n",
    "#                 if os.path.exists(csv_file_path):\n",
    "#                     # Load the existing CSV file into a DataFrame\n",
    "#                     df = pd.read_csv(csv_file_path)\n",
    "#                     # Append the new data to the DataFrame\n",
    "#                     df = pd.concat([df, pd.DataFrame([beer_data])], ignore_index=True)\n",
    "#                 else:\n",
    "#                     # Create a new DataFrame if the CSV file does not exist\n",
    "#                     df = pd.DataFrame([beer_data])\n",
    "\n",
    "#                 # Save the DataFrame to a CSV file\n",
    "#                 df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# # Run the scraper\n",
    "# scrape_untappd()\n",
    "\n",
    "# # Close the driver\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapper 2: Scrape Top Rated Results\n",
    "Scrape from `https://untappd.com/brewery/top_rated?country={country}&brewery_type={brewery_type}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "# ============\n",
    "# === BEAUTIFULSOUP ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === DOTENV ===\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === OS ===\n",
    "import os\n",
    "\n",
    "# === PANDAS ===\n",
    "import pandas as pd\n",
    "\n",
    "# === PICKLE ===\n",
    "import pickle\n",
    "\n",
    "# === SELENIUM ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === TIME ===\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# SAVE LOGIN DETAILS\n",
    "# ============\n",
    "\n",
    "COOKIES_FILE_PATH = 'cookies.pkl'\n",
    "\n",
    "def save_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'rb') as file:\n",
    "        cookies = pickle.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://untappd.com\"\n",
    "LOGIN_URL = f\"{BASE_URL}/login\"\n",
    "HOME_URL = f\"{BASE_URL}/home\"\n",
    "\n",
    "# Credentials\n",
    "load_dotenv()\n",
    "USERNAME = os.getenv('USERNAME')\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "\n",
    "# Countries\n",
    "COUNTRIES = [\n",
    "    \"united states\",\n",
    "    \"canada\",\n",
    "    \"england\",\n",
    "    \"scotland\",\n",
    "    \"ireland\",\n",
    "    \"northern ireland\",\n",
    "    # \"wales\",\n",
    "    # \"germany\",\n",
    "    # \"belgium\",\n",
    "    # \"austria\",\n",
    "    # \"netherlands\",\n",
    "    # \"poland\",\n",
    "    # \"czech republic\",\n",
    "    # \"norway\",\n",
    "    # \"france\",\n",
    "    # \"denmark\",\n",
    "    # \"sweden\"\n",
    "]\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "DIRECTORY = \"../Results/Untappd Top Rated\"\n",
    "\n",
    "def last_csv(countries):\n",
    "    csv_files = []\n",
    "    \n",
    "    for country in countries:\n",
    "        # Construct the full file name with .csv extension\n",
    "        file_name = f\"{country}.csv\"\n",
    "        file_path = os.path.join(DIRECTORY, file_name)\n",
    "        \n",
    "        # Check if the file exists in the directory\n",
    "        if os.path.exists(file_path):\n",
    "            csv_files.append(file_path)  # Store the full path instead of just the file name\n",
    "    \n",
    "    # Return the path of the last CSV file found, or None if no files were found\n",
    "    return csv_files[-1] if csv_files else None\n",
    "\n",
    "def last_record(file_path):\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the DataFrame is not empty\n",
    "            if not df.empty:\n",
    "                # Return the last row of the DataFrame\n",
    "                return df.iloc[-1]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "def login():\n",
    "    # MANUAL STEPS\n",
    "    # ============\n",
    "    # 1. Verify the CAPTCHA\n",
    "    # 2. Press the Enter key to continue after verifying the CAPTCHA\n",
    "    # 3. Press \"Enter\" key in VSCode terminal to continue (ensures that login has been successful)\n",
    "\n",
    "    driver.get(LOGIN_URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Load cookies if they exist and check if already logged in\n",
    "    if os.path.exists(COOKIES_FILE_PATH):\n",
    "        load_cookies(driver)\n",
    "        driver.get(HOME_URL)  # Navigate to home to check if login was successful\n",
    "        if driver.current_url == HOME_URL:\n",
    "            print(\"Logged in using saved cookies\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Failed to log in with saved cookies. Proceeding with manual login.\")\n",
    "\n",
    "    # Enter username\n",
    "    username_input = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    username_input.send_keys(USERNAME)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "    password_input.send_keys(PASSWORD)\n",
    "\n",
    "    try:\n",
    "        # Wait for the CAPTCHA and solve it manually if it appears\n",
    "        captcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "        if captcha_frame:\n",
    "            print(\"Please solve the CAPTCHA manually if it appears, then press Enter to continue...\")\n",
    "            input()\n",
    "    except:\n",
    "        # No CAPTCHA present, continue with login\n",
    "        pass\n",
    "\n",
    "    # Wait until redirected to HOME_URL\n",
    "    wait.until(EC.url_to_be(HOME_URL))\n",
    "    print(\"Login successful, redirected to home\")\n",
    "\n",
    "    # Save cookies after successful login\n",
    "    save_cookies(driver)\n",
    "\n",
    "def get_soup():\n",
    "    time.sleep(2)  # Ensure the page has fully loaded\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def click_show_more(class_name):\n",
    "    time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    # Check if there is an \"announcement\" modal\n",
    "    try:\n",
    "        announcement_modal = driver.find_element(By.CLASS_NAME, 'announcement')\n",
    "        if announcement_modal:\n",
    "            track_button = announcement_modal.find_element(By.CLASS_NAME, 'track-click')\n",
    "            track_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "            # Find and click the \"Show More\" button\n",
    "            show_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, class_name))\n",
    "            )\n",
    "            ActionChains(driver).move_to_element(show_more_button).click().perform()\n",
    "            time.sleep(2)  # Ensure the new content has loaded\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "def parse_breweries(soup):\n",
    "    breweries = []\n",
    "    for brewery in soup.select('div.beer-details'):\n",
    "        name = brewery.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + brewery.select_one('p.name a')['href']\n",
    "        breweries.append((name, link))\n",
    "    print(f\"Found {len(breweries)} breweries\")\n",
    "    return breweries\n",
    "\n",
    "def parse_breweries_details(soup):\n",
    "    country = soup.select_one('p.brewery').text.strip() if soup.select_one('p.brewery') else \"\"\n",
    "    return country\n",
    "\n",
    "def parse_beers(soup):\n",
    "    beers = []\n",
    "    for beer in soup.select('div.beer-details'):\n",
    "        name = beer.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + beer.select_one('p.name a')['href']\n",
    "        beers.append((name, link))\n",
    "    print(f\"Found {len(beers)} beers\")\n",
    "    return beers\n",
    "\n",
    "def parse_beer_details(soup):\n",
    "    abv = soup.select_one('p.abv').text.strip() if soup.select_one('p.abv') else \"\"\n",
    "    drink_category = soup.select_one('div.name p.style').text.strip() if soup.select_one('div.name p.style') else \"\"\n",
    "    latest_review_date = soup.select_one('div.bottom a.time')['data-gregtime'] if soup.select_one('div.bottom a.time') else \"\"\n",
    "    official_description = soup.select_one('div.beer-descrption-read-less').text.strip() if soup.select_one('div.beer-descrption-read-less') else \"\"\n",
    "    if official_description.endswith(\" Show Less\"):\n",
    "        official_description = official_description[:-10]  # Remove the last 10 characters (\" Show Less\")\n",
    "    photo_link = soup.select_one('a.label.image-big')['data-image'] if soup.select_one('a.label.image-big') else \"\"\n",
    "    return abv, drink_category, latest_review_date, official_description, photo_link\n",
    "\n",
    "def scrape_untappd():\n",
    "    login()  # Perform login\n",
    "\n",
    "    # Get all brewery types except \"all\"\n",
    "    driver.get(f\"{BASE_URL}/brewery/top_rated?country=&brewery_type=\")\n",
    "    soup = get_soup()\n",
    "    brewery_types = [option['data-value-slug'] for option in soup.select('select#filter_picker option') if option['value'] != 'all']\n",
    "\n",
    "    # Extract last scraped details\n",
    "    last_csv_file = last_csv(COUNTRIES)  # Get the last CSV file path\n",
    "    last_row = last_record(last_csv_file)  # Get the last row from the last CSV file\n",
    "\n",
    "    start_brewery_idx = 0\n",
    "    start_beer_idx = 0\n",
    "    start_country_idx = 0\n",
    "\n",
    "    if last_csv_file is not None and last_row is not None:\n",
    "        # Continue from last scraped brewery and beer\n",
    "        start_brewery_idx = last_row['Brewery Index'] - 1\n",
    "        start_beer_idx = last_row['Beer Index']\n",
    "        # Find the index of the country to start from\n",
    "        for idx, country in enumerate(COUNTRIES):\n",
    "            if country in last_csv_file:\n",
    "                start_country_idx = idx\n",
    "                break\n",
    "\n",
    "    # Initialize global brewery index counter\n",
    "    global_brewery_idx = 0\n",
    "\n",
    "    for country_idx, search_country in enumerate(COUNTRIES[start_country_idx:], start=start_country_idx):\n",
    "        # Check if \"search_country\" contains \" \", replace with \"-\" if it does\n",
    "        search_country_formatted = search_country\n",
    "        if \" \" in search_country:\n",
    "            search_country_formatted = search_country.replace(\" \", \"-\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Scraping country: {search_country}\")\n",
    "\n",
    "        for brewery_type in brewery_types:\n",
    "            print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
    "            print(f\"Scraping brewery type: {brewery_type}\")\n",
    "            SEARCH_URL = f\"{BASE_URL}/brewery/top_rated?country={search_country_formatted}&brewery_type={brewery_type}\"\n",
    "            driver.get(SEARCH_URL)\n",
    "            time.sleep(2)  # Wait for the page to load\n",
    "            soup = get_soup()\n",
    "            breweries = parse_breweries(soup)\n",
    "\n",
    "            # Determine if we need to skip this type based on the global index\n",
    "            if global_brewery_idx + len(breweries) <= start_brewery_idx:\n",
    "                global_brewery_idx += len(breweries)\n",
    "                continue\n",
    "\n",
    "            beer_data = []\n",
    "\n",
    "            for brewery_idx, (brewery_name, brewery_url) in enumerate(breweries):\n",
    "                if global_brewery_idx < start_brewery_idx:\n",
    "                    global_brewery_idx += 1\n",
    "                    continue\n",
    "\n",
    "                print(\"==================================================\")\n",
    "                print(f\"Scraping brewery {brewery_idx + 1}: {brewery_name}\")\n",
    "\n",
    "                # Step B: Scrape beers from each brewery\n",
    "                driver.get(f\"{brewery_url}/beer\")\n",
    "\n",
    "                click_show_more('more-list-items')  # Ensure all beers are loaded\n",
    "\n",
    "                soup = get_soup()\n",
    "                beers = parse_beers(soup)\n",
    "\n",
    "                # Step C: Scrape brewery details\n",
    "                country = parse_breweries_details(soup)\n",
    "                for beer_idx, (beer_name, beer_url) in enumerate(beers):\n",
    "                    if global_brewery_idx == start_brewery_idx and beer_idx < start_beer_idx:\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Scraping beer {beer_idx + 1}/{len(beers)}: {beer_name}\")\n",
    "\n",
    "                    # Step D: Scrape beer details\n",
    "                    driver.get(beer_url)\n",
    "                    beer_soup = get_soup()\n",
    "                    abv, drink_category, latest_review_date, official_description, photo_link = parse_beer_details(beer_soup)\n",
    "\n",
    "                    # Collect beer data\n",
    "                    beer_data = {\n",
    "                        'Country': country,\n",
    "                        'Brewery Index': global_brewery_idx + 1,\n",
    "                        'Beer Index': beer_idx + 1,\n",
    "                        'Scraping Remarks / Errors': '',  # Add any remarks if needed\n",
    "                        'Date of latest review': latest_review_date,\n",
    "                        'Expression Name': beer_name,\n",
    "                        'Producer': brewery_name,\n",
    "                        'Bottler': '', # Leave blank for now\n",
    "                        'Country of Origin': country,\n",
    "                        'Drink Type': 'Beer',\n",
    "                        'Drink Category': drink_category,\n",
    "                        'Age': '',\n",
    "                        'ABV': abv,\n",
    "                        '88B Website Review Link': '', # Leave blank for now\n",
    "                        'Official Description': official_description,\n",
    "                        'Producer Website Link': '',\n",
    "                        'Photo Link': photo_link\n",
    "                    }\n",
    "\n",
    "                    # Step E: Save the collected data to CSV\n",
    "                    # Save for each beer scraped\n",
    "\n",
    "                    csv_file_path = f'{DIRECTORY}/{search_country}.csv'\n",
    "\n",
    "                    # Check if the CSV file exists\n",
    "                    if os.path.exists(csv_file_path):\n",
    "                        # Load the existing CSV file into a DataFrame\n",
    "                        df = pd.read_csv(csv_file_path)\n",
    "                        # Append the new data to the DataFrame\n",
    "                        df = pd.concat([df, pd.DataFrame([beer_data])], ignore_index=True)\n",
    "                    else:\n",
    "                        # Create a new DataFrame if the CSV file does not exist\n",
    "                        df = pd.DataFrame([beer_data])\n",
    "\n",
    "                    # Save the DataFrame to a CSV file\n",
    "                    df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "                # Increment the global brewery index\n",
    "                global_brewery_idx += 1\n",
    "\n",
    "# Run the scraper\n",
    "scrape_untappd()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
