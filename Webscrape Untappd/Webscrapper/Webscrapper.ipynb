{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapper 1: Scrape Search Results\n",
    "Scrape from `https://untappd.com/search?q={country}&type=brewery&sort=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "# ============\n",
    "# === BEAUTIFULSOUP ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === DOTENV ===\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === OS ===\n",
    "import os\n",
    "\n",
    "# === PANDAS ===\n",
    "import pandas as pd\n",
    "\n",
    "# === PICKLE ===\n",
    "import pickle\n",
    "\n",
    "# === SELENIUM ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === TIME ===\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# SAVE LOGIN DETAILS\n",
    "# ============\n",
    "\n",
    "COOKIES_FILE_PATH = 'cookies.pkl'\n",
    "\n",
    "def save_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'rb') as file:\n",
    "        cookies = pickle.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://untappd.com\"\n",
    "LOGIN_URL = f\"{BASE_URL}/login\"\n",
    "HOME_URL = f\"{BASE_URL}/home\"\n",
    "\n",
    "# Credentials\n",
    "load_dotenv()\n",
    "USERNAME = os.getenv('USERNAME')\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "\n",
    "# Countries\n",
    "COUNTRIES = [\n",
    "    # \"singapore\",\n",
    "    # \"laos\",\n",
    "    # \"malaysia\",\n",
    "    # \"cambodia\",\n",
    "    # \"indonesia\",\n",
    "    # \"hong kong\",\n",
    "    # \"philippines\",\n",
    "    # \"taiwan\",\n",
    "    # \"vietnam\",\n",
    "    # \"israel\",\n",
    "    # \"india\",\n",
    "    # \"thailand\",\n",
    "    # \"korea\",\n",
    "    # \"japan\",\n",
    "    # \"china\",\n",
    "    # \"new zealand\",\n",
    "    # \"australia\"\n",
    "]\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "DIRECTORY = \"../Results/Untappd Search\"\n",
    "\n",
    "def last_csv(countries):\n",
    "    csv_files = []\n",
    "    \n",
    "    for country in countries:\n",
    "        # Construct the full file name with .csv extension\n",
    "        file_name = f\"{country}.csv\"\n",
    "        file_path = os.path.join(DIRECTORY, file_name)\n",
    "        \n",
    "        # Check if the file exists in the directory\n",
    "        if os.path.exists(file_path):\n",
    "            csv_files.append(file_path)  # Store the full path instead of just the file name\n",
    "    \n",
    "    # Return the path of the last CSV file found, or None if no files were found\n",
    "    return csv_files[-1] if csv_files else None\n",
    "\n",
    "def last_record(file_path):\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the DataFrame is not empty\n",
    "            if not df.empty:\n",
    "                # Return the last row of the DataFrame\n",
    "                return df.iloc[-1]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "def login():\n",
    "    # MANUAL STEPS\n",
    "    # ============\n",
    "    # 1. Verify the CAPTCHA\n",
    "    # 2. Press the Enter key to continue after verifying the CAPTCHA\n",
    "    # 3. Press \"Enter\" key in VSCode terminal to continue (ensures that login has been successful)\n",
    "\n",
    "    driver.get(LOGIN_URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Load cookies if they exist and check if already logged in\n",
    "    if os.path.exists(COOKIES_FILE_PATH):\n",
    "        load_cookies(driver)\n",
    "        driver.get(HOME_URL)  # Navigate to home to check if login was successful\n",
    "        if driver.current_url == HOME_URL:\n",
    "            print(\"Logged in using saved cookies\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Failed to log in with saved cookies. Proceeding with manual login.\")\n",
    "\n",
    "    # Enter username\n",
    "    username_input = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    username_input.send_keys(USERNAME)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "    password_input.send_keys(PASSWORD)\n",
    "\n",
    "    try:\n",
    "        # Wait for the CAPTCHA and solve it manually if it appears\n",
    "        captcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "        if captcha_frame:\n",
    "            print(\"Please solve the CAPTCHA manually if it appears, then press Enter to continue...\")\n",
    "            input()\n",
    "    except:\n",
    "        # No CAPTCHA present, continue with login\n",
    "        pass\n",
    "\n",
    "    # Wait until redirected to HOME_URL\n",
    "    wait.until(EC.url_to_be(HOME_URL))\n",
    "    print(\"Login successful, redirected to home\")\n",
    "\n",
    "    # Save cookies after successful login\n",
    "    save_cookies(driver)\n",
    "\n",
    "def get_soup():\n",
    "    time.sleep(2)  # Ensure the page has fully loaded\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def click_show_more(class_name):\n",
    "    time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    # Check if there is an \"announcement\" modal\n",
    "    try:\n",
    "        announcement_modal = driver.find_element(By.CLASS_NAME, 'announcement')\n",
    "        if announcement_modal:\n",
    "            track_button = announcement_modal.find_element(By.CLASS_NAME, 'track-click')\n",
    "            track_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "            # Find and click the \"Show More\" button\n",
    "            show_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, class_name))\n",
    "            )\n",
    "            ActionChains(driver).move_to_element(show_more_button).click().perform()\n",
    "            time.sleep(2)  # Ensure the new content has loaded\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "def parse_breweries(soup):\n",
    "    breweries = []\n",
    "    for brewery in soup.select('div.beer-details'):\n",
    "        name = brewery.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + brewery.select_one('p.name a')['href']\n",
    "        breweries.append((name, link))\n",
    "    print(f\"Found {len(breweries)} breweries\")\n",
    "    return breweries\n",
    "\n",
    "def parse_breweries_details(soup):\n",
    "    country = soup.select_one('p.brewery').text.strip() if soup.select_one('p.brewery') else \"\"\n",
    "    return country\n",
    "\n",
    "def parse_beers(soup):\n",
    "    beers = []\n",
    "    for beer in soup.select('div.beer-details'):\n",
    "        name = beer.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + beer.select_one('p.name a')['href']\n",
    "        beers.append((name, link))\n",
    "    print(f\"Found {len(beers)} beers\")\n",
    "    return beers\n",
    "\n",
    "def parse_beer_details(soup):\n",
    "    abv = soup.select_one('p.abv').text.strip() if soup.select_one('p.abv') else \"\"\n",
    "    drink_category = soup.select_one('div.name p.style').text.strip() if soup.select_one('div.name p.style') else \"\"\n",
    "    latest_review_date = soup.select_one('div.bottom a.time')['data-gregtime'] if soup.select_one('div.bottom a.time') else \"\"\n",
    "    official_description = soup.select_one('div.beer-descrption-read-less').text.strip() if soup.select_one('div.beer-descrption-read-less') else \"\"\n",
    "    if official_description.endswith(\" Show Less\"):\n",
    "        official_description = official_description[:-10]  # Remove the last 10 characters (\" Show Less\")\n",
    "    photo_link = soup.select_one('a.label.image-big')['data-image'] if soup.select_one('a.label.image-big') else \"\"\n",
    "    return abv, drink_category, latest_review_date, official_description, photo_link\n",
    "\n",
    "def scrape_untappd():\n",
    "    login()  # Perform login\n",
    "\n",
    "    # Extract last scraped details\n",
    "    last_csv_file = last_csv(COUNTRIES)  # Get the last CSV file path\n",
    "    last_row = last_record(last_csv_file)  # Get the last row from the last CSV file\n",
    "\n",
    "    start_brewery_idx = 0\n",
    "    start_beer_idx = 0\n",
    "    start_country_idx = 0\n",
    "\n",
    "    if last_csv_file is not None and last_row is not None:\n",
    "        # Continue from last scraped brewery and beer\n",
    "        start_brewery_idx = last_row['Brewery Index'] - 1\n",
    "        start_beer_idx = last_row['Beer Index']\n",
    "        country = last_csv_file.split('/')[-1].split('.')[0]\n",
    "        # Find the index of the country to start from\n",
    "        for idx, country in enumerate(COUNTRIES):\n",
    "            if country in last_csv_file:\n",
    "                start_country_idx = idx\n",
    "                break\n",
    "\n",
    "    for country_idx, search_country in enumerate(COUNTRIES[start_country_idx:], start=start_country_idx):\n",
    "        # Check if \"search_country\" contains \" \", replace with \"+\" if it does\n",
    "        search_country_formatted = search_country\n",
    "        if \" \" in search_country:\n",
    "            search_country_formatted = search_country.replace(\" \", \"+\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Scraping country: {search_country}\")\n",
    "\n",
    "        SEARCH_URL = f\"{BASE_URL}/search?q={search_country_formatted}&type=brewery&sort=\"\n",
    "        driver.get(SEARCH_URL)\n",
    "        click_show_more('more_search')  # Ensure all breweries are loaded\n",
    "        soup = get_soup()\n",
    "        breweries = parse_breweries(soup)\n",
    "\n",
    "        beer_data = []\n",
    "\n",
    "        for brewery_idx, (brewery_name, brewery_url) in enumerate(breweries):\n",
    "            if country_idx == start_country_idx and brewery_idx < start_brewery_idx:\n",
    "                continue\n",
    "\n",
    "            print(\"==================================================\")\n",
    "            print(f\"Scraping brewery {brewery_idx + 1}/{len(breweries)}: {brewery_name}\")\n",
    "\n",
    "            # Step B: Scrape beers from each brewery\n",
    "            driver.get(f\"{brewery_url}/beer\")\n",
    "            click_show_more('more-list-items')  # Ensure all beers are loaded\n",
    "            soup = get_soup()\n",
    "            beers = parse_beers(soup)\n",
    "\n",
    "            # Step C: Scrape brewery details\n",
    "            country = parse_breweries_details(soup)\n",
    "            for beer_idx, (beer_name, beer_url) in enumerate(beers):\n",
    "                if country_idx == start_country_idx and brewery_idx == start_brewery_idx and beer_idx < start_beer_idx:\n",
    "                    continue\n",
    "\n",
    "                print(f\"Scraping beer {beer_idx + 1}/{len(beers)}: {beer_name}\")\n",
    "\n",
    "                # Step D: Scrape beer details\n",
    "                driver.get(beer_url)\n",
    "                beer_soup = get_soup()\n",
    "                abv, drink_category, latest_review_date, official_description, photo_link = parse_beer_details(beer_soup)\n",
    "\n",
    "                # Collect beer data\n",
    "                beer_data = {\n",
    "                    'Brewery Index': brewery_idx + 1,\n",
    "                    'Beer Index': beer_idx + 1,\n",
    "                    'Scraping Remarks / Errors': '',  # Add any remarks if needed\n",
    "                    'Date of latest review': latest_review_date,\n",
    "                    'Expression Name': beer_name,\n",
    "                    'Producer': brewery_name,\n",
    "                    'Bottler': '', # Leave blank for now\n",
    "                    'Country of Origin': country,\n",
    "                    'Drink Type': 'Beer',\n",
    "                    'Drink Category': drink_category,\n",
    "                    'Age': '',\n",
    "                    'ABV': abv,\n",
    "                    '88B Website Review Link': '', # Leave blank for now\n",
    "                    'Official Description': official_description,\n",
    "                    'Producer Website Link': '',\n",
    "                    'Photo Link': photo_link\n",
    "                }\n",
    "\n",
    "                # Step E: Save the collected data to CSV\n",
    "                # Save for each beer scraped\n",
    "\n",
    "                csv_file_path = f'{DIRECTORY}/{search_country}.csv'\n",
    "\n",
    "                # Check if the CSV file exists\n",
    "                if os.path.exists(csv_file_path):\n",
    "                    # Load the existing CSV file into a DataFrame\n",
    "                    df = pd.read_csv(csv_file_path)\n",
    "                    # Append the new data to the DataFrame\n",
    "                    df = pd.concat([df, pd.DataFrame([beer_data])], ignore_index=True)\n",
    "                else:\n",
    "                    # Create a new DataFrame if the CSV file does not exist\n",
    "                    df = pd.DataFrame([beer_data])\n",
    "\n",
    "                # Save the DataFrame to a CSV file\n",
    "                df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_untappd()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapper 2: Scrape Top Rated Results\n",
    "Scrape from `https://untappd.com/brewery/top_rated?country={country}&brewery_type={brewery_type}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in using saved cookies\n",
      "----------------------------------------\n",
      "Scraping country: united states\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Scraping brewery type: macro_brewery\n",
      "Found 50 breweries\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Scraping brewery type: micro_brewery\n",
      "Found 50 breweries\n",
      "==================================================\n",
      "Scraping brewery 34: Boiler Brewing Co.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b_/6l6rrpzs5yqd2b5_sl147sq40000gn/T/ipykernel_41130/2101879992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;31m# Run the scraper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m \u001b[0mscrape_untappd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;31m# Close the driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b_/6l6rrpzs5yqd2b5_sl147sq40000gn/T/ipykernel_41130/2101879992.py\u001b[0m in \u001b[0;36mscrape_untappd\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{brewery_url}/beer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mclick_show_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'more-list-items'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure all beers are loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b_/6l6rrpzs5yqd2b5_sl147sq40000gn/T/ipykernel_41130/2101879992.py\u001b[0m in \u001b[0;36mclick_show_more\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    188\u001b[0m             )\n\u001b[1;32m    189\u001b[0m             \u001b[0mActionChains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_more_button\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure the new content has loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "# ============\n",
    "# === BEAUTIFULSOUP ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === DOTENV ===\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === OS ===\n",
    "import os\n",
    "\n",
    "# === PANDAS ===\n",
    "import pandas as pd\n",
    "\n",
    "# === PICKLE ===\n",
    "import pickle\n",
    "\n",
    "# === SELENIUM ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === TIME ===\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# SAVE LOGIN DETAILS\n",
    "# ============\n",
    "\n",
    "COOKIES_FILE_PATH = 'cookies.pkl'\n",
    "\n",
    "def save_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'rb') as file:\n",
    "        cookies = pickle.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://untappd.com\"\n",
    "LOGIN_URL = f\"{BASE_URL}/login\"\n",
    "HOME_URL = f\"{BASE_URL}/home\"\n",
    "\n",
    "# Credentials\n",
    "load_dotenv()\n",
    "USERNAME = os.getenv('USERNAME')\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "\n",
    "# Countries\n",
    "COUNTRIES = [\n",
    "    \"united states\",\n",
    "    \"canada\",\n",
    "    \"england\",\n",
    "    \"scotland\",\n",
    "    \"ireland\",\n",
    "    \"northern ireland\",\n",
    "    # \"wales\",\n",
    "    # \"germany\",\n",
    "    # \"belgium\",\n",
    "    # \"austria\",\n",
    "    # \"netherlands\",\n",
    "    # \"poland\",\n",
    "    # \"czech republic\",\n",
    "    # \"norway\",\n",
    "    # \"france\",\n",
    "    # \"denmark\",\n",
    "    # \"sweden\"\n",
    "]\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "DIRECTORY = \"../Results/Untappd Top Rated\"\n",
    "\n",
    "def last_csv(countries):\n",
    "    csv_files = []\n",
    "    \n",
    "    for country in countries:\n",
    "        # Construct the full file name with .csv extension\n",
    "        file_name = f\"{country}.csv\"\n",
    "        file_path = os.path.join(DIRECTORY, file_name)\n",
    "        \n",
    "        # Check if the file exists in the directory\n",
    "        if os.path.exists(file_path):\n",
    "            csv_files.append(file_path)  # Store the full path instead of just the file name\n",
    "    \n",
    "    # Return the path of the last CSV file found, or None if no files were found\n",
    "    return csv_files[-1] if csv_files else None\n",
    "\n",
    "def last_record(file_path):\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the DataFrame is not empty\n",
    "            if not df.empty:\n",
    "                # Return the last row of the DataFrame\n",
    "                return df.iloc[-1]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "def login():\n",
    "    # MANUAL STEPS\n",
    "    # ============\n",
    "    # 1. Verify the CAPTCHA\n",
    "    # 2. Press the Enter key to continue after verifying the CAPTCHA\n",
    "    # 3. Press \"Enter\" key in VSCode terminal to continue (ensures that login has been successful)\n",
    "\n",
    "    driver.get(LOGIN_URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Load cookies if they exist and check if already logged in\n",
    "    if os.path.exists(COOKIES_FILE_PATH):\n",
    "        load_cookies(driver)\n",
    "        driver.get(HOME_URL)  # Navigate to home to check if login was successful\n",
    "        if driver.current_url == HOME_URL:\n",
    "            print(\"Logged in using saved cookies\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Failed to log in with saved cookies. Proceeding with manual login.\")\n",
    "\n",
    "    # Enter username\n",
    "    username_input = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    username_input.send_keys(USERNAME)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "    password_input.send_keys(PASSWORD)\n",
    "\n",
    "    try:\n",
    "        # Wait for the CAPTCHA and solve it manually if it appears\n",
    "        captcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "        if captcha_frame:\n",
    "            print(\"Please solve the CAPTCHA manually if it appears, then press Enter to continue...\")\n",
    "            input()\n",
    "    except:\n",
    "        # No CAPTCHA present, continue with login\n",
    "        pass\n",
    "\n",
    "    # Wait until redirected to HOME_URL\n",
    "    wait.until(EC.url_to_be(HOME_URL))\n",
    "    print(\"Login successful, redirected to home\")\n",
    "\n",
    "    # Save cookies after successful login\n",
    "    save_cookies(driver)\n",
    "\n",
    "def get_soup():\n",
    "    time.sleep(2)  # Ensure the page has fully loaded\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def click_show_more(class_name):\n",
    "    time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    # Check if there is an \"announcement\" modal\n",
    "    try:\n",
    "        announcement_modal = driver.find_element(By.CLASS_NAME, 'announcement')\n",
    "        if announcement_modal:\n",
    "            track_button = announcement_modal.find_element(By.CLASS_NAME, 'track-click')\n",
    "            track_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "            # Find and click the \"Show More\" button\n",
    "            show_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, class_name))\n",
    "            )\n",
    "            ActionChains(driver).move_to_element(show_more_button).click().perform()\n",
    "            time.sleep(2)  # Ensure the new content has loaded\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "def parse_breweries(soup):\n",
    "    breweries = []\n",
    "    for brewery in soup.select('div.beer-details'):\n",
    "        name = brewery.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + brewery.select_one('p.name a')['href']\n",
    "        breweries.append((name, link))\n",
    "    print(f\"Found {len(breweries)} breweries\")\n",
    "    return breweries\n",
    "\n",
    "def parse_breweries_details(soup):\n",
    "    country = soup.select_one('p.brewery').text.strip() if soup.select_one('p.brewery') else \"\"\n",
    "    return country\n",
    "\n",
    "def parse_beers(soup):\n",
    "    beers = []\n",
    "    for beer in soup.select('div.beer-details'):\n",
    "        name = beer.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + beer.select_one('p.name a')['href']\n",
    "        beers.append((name, link))\n",
    "    print(f\"Found {len(beers)} beers\")\n",
    "    return beers\n",
    "\n",
    "def parse_beer_details(soup):\n",
    "    abv = soup.select_one('p.abv').text.strip() if soup.select_one('p.abv') else \"\"\n",
    "    drink_category = soup.select_one('div.name p.style').text.strip() if soup.select_one('div.name p.style') else \"\"\n",
    "    latest_review_date = soup.select_one('div.bottom a.time')['data-gregtime'] if soup.select_one('div.bottom a.time') else \"\"\n",
    "    official_description = soup.select_one('div.beer-descrption-read-less').text.strip() if soup.select_one('div.beer-descrption-read-less') else \"\"\n",
    "    if official_description.endswith(\" Show Less\"):\n",
    "        official_description = official_description[:-10]  # Remove the last 10 characters (\" Show Less\")\n",
    "    photo_link = soup.select_one('a.label.image-big')['data-image'] if soup.select_one('a.label.image-big') else \"\"\n",
    "    return abv, drink_category, latest_review_date, official_description, photo_link\n",
    "\n",
    "def scrape_untappd():\n",
    "    login()  # Perform login\n",
    "\n",
    "    # Get all brewery types except \"all\"\n",
    "    driver.get(f\"{BASE_URL}/brewery/top_rated?country=&brewery_type=\")\n",
    "    soup = get_soup()\n",
    "    brewery_types = [option['data-value-slug'] for option in soup.select('select#filter_picker option') if option['value'] != 'all']\n",
    "\n",
    "    # Extract last scraped details\n",
    "    last_csv_file = last_csv(COUNTRIES)  # Get the last CSV file path\n",
    "    last_row = last_record(last_csv_file)  # Get the last row from the last CSV file\n",
    "\n",
    "    start_brewery_idx = 0\n",
    "    start_beer_idx = 0\n",
    "    start_country_idx = 0\n",
    "\n",
    "    if last_csv_file is not None and last_row is not None:\n",
    "        # Continue from last scraped brewery and beer\n",
    "        start_brewery_idx = last_row['Brewery Index'] - 1\n",
    "        start_beer_idx = last_row['Beer Index']\n",
    "        country = last_csv_file.split('/')[-1].split('.')[0]\n",
    "        # Find the index of the country to start from\n",
    "        for idx, country in enumerate(COUNTRIES):\n",
    "            if country in last_csv_file:\n",
    "                start_country_idx = idx\n",
    "                break\n",
    "\n",
    "    # Initialize global brewery index counter\n",
    "    global_brewery_idx = 0\n",
    "\n",
    "    for country_idx, search_country in enumerate(COUNTRIES[start_country_idx:], start=start_country_idx):\n",
    "        # Check if \"search_country\" contains \" \", replace with \"-\" if it does\n",
    "        search_country_formatted = search_country\n",
    "        if \" \" in search_country:\n",
    "            search_country_formatted = search_country.replace(\" \", \"-\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Scraping country: {search_country}\")\n",
    "\n",
    "        for brewery_type in brewery_types:\n",
    "            print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
    "            print(f\"Scraping brewery type: {brewery_type}\")\n",
    "            SEARCH_URL = f\"{BASE_URL}/brewery/top_rated?country={search_country_formatted}&brewery_type={brewery_type}\"\n",
    "            driver.get(SEARCH_URL)\n",
    "            time.sleep(2)  # Wait for the page to load\n",
    "            soup = get_soup()\n",
    "            breweries = parse_breweries(soup)\n",
    "\n",
    "            # Determine if we need to skip this type based on the global index\n",
    "            if global_brewery_idx + len(breweries) <= start_brewery_idx:\n",
    "                global_brewery_idx += len(breweries)\n",
    "                continue\n",
    "\n",
    "            beer_data = []\n",
    "\n",
    "            for brewery_idx, (brewery_name, brewery_url) in enumerate(breweries):\n",
    "                if global_brewery_idx < start_brewery_idx:\n",
    "                    global_brewery_idx += 1\n",
    "                    continue\n",
    "\n",
    "                print(\"==================================================\")\n",
    "                print(f\"Scraping brewery {brewery_idx + 1}: {brewery_name}\")\n",
    "\n",
    "                # Step B: Scrape beers from each brewery\n",
    "                driver.get(f\"{brewery_url}/beer\")\n",
    "\n",
    "                click_show_more('more-list-items')  # Ensure all beers are loaded\n",
    "\n",
    "                soup = get_soup()\n",
    "                beers = parse_beers(soup)\n",
    "\n",
    "                # Step C: Scrape brewery details\n",
    "                country = parse_breweries_details(soup)\n",
    "                for beer_idx, (beer_name, beer_url) in enumerate(beers):\n",
    "                    if global_brewery_idx == start_brewery_idx and beer_idx < start_beer_idx:\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Scraping beer {beer_idx + 1}/{len(beers)}: {beer_name}\")\n",
    "\n",
    "                    # Step D: Scrape beer details\n",
    "                    driver.get(beer_url)\n",
    "                    beer_soup = get_soup()\n",
    "                    abv, drink_category, latest_review_date, official_description, photo_link = parse_beer_details(beer_soup)\n",
    "\n",
    "                    # Collect beer data\n",
    "                    beer_data = {\n",
    "                        'Brewery Index': global_brewery_idx + 1,\n",
    "                        'Beer Index': beer_idx + 1,\n",
    "                        'Scraping Remarks / Errors': '',  # Add any remarks if needed\n",
    "                        'Date of latest review': latest_review_date,\n",
    "                        'Expression Name': beer_name,\n",
    "                        'Producer': brewery_name,\n",
    "                        'Bottler': '', # Leave blank for now\n",
    "                        'Country of Origin': country,\n",
    "                        'Drink Type': 'Beer',\n",
    "                        'Drink Category': drink_category,\n",
    "                        'Age': '',\n",
    "                        'ABV': abv,\n",
    "                        '88B Website Review Link': '', # Leave blank for now\n",
    "                        'Official Description': official_description,\n",
    "                        'Producer Website Link': '',\n",
    "                        'Photo Link': photo_link\n",
    "                    }\n",
    "\n",
    "                    # Step E: Save the collected data to CSV\n",
    "                    # Save for each beer scraped\n",
    "\n",
    "                    csv_file_path = f'{DIRECTORY}/{search_country}.csv'\n",
    "\n",
    "                    # Check if the CSV file exists\n",
    "                    if os.path.exists(csv_file_path):\n",
    "                        # Load the existing CSV file into a DataFrame\n",
    "                        df = pd.read_csv(csv_file_path)\n",
    "                        # Append the new data to the DataFrame\n",
    "                        df = pd.concat([df, pd.DataFrame([beer_data])], ignore_index=True)\n",
    "                    else:\n",
    "                        # Create a new DataFrame if the CSV file does not exist\n",
    "                        df = pd.DataFrame([beer_data])\n",
    "\n",
    "                    # Save the DataFrame to a CSV file\n",
    "                    df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "                # Increment the global brewery index\n",
    "                global_brewery_idx += 1\n",
    "\n",
    "# Run the scraper\n",
    "scrape_untappd()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
