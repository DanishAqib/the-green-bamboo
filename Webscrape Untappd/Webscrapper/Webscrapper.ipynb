{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapper 1: Scrape Search Results\n",
    "Scrape from `https://untappd.com/search?q={country}&type=brewery&sort=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "# ============\n",
    "# === BEAUTIFULSOUP ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === DOTENV ===\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === OS ===\n",
    "import os\n",
    "\n",
    "# === PANDAS ===\n",
    "import pandas as pd\n",
    "\n",
    "# === PICKLE ===\n",
    "import pickle\n",
    "\n",
    "# === SELENIUM ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === TIME ===\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# SAVE LOGIN DETAILS\n",
    "# ============\n",
    "\n",
    "COOKIES_FILE_PATH = 'cookies.pkl'\n",
    "\n",
    "def save_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'rb') as file:\n",
    "        cookies = pickle.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://untappd.com\"\n",
    "LOGIN_URL = f\"{BASE_URL}/login\"\n",
    "HOME_URL = f\"{BASE_URL}/home\"\n",
    "\n",
    "# Credentials\n",
    "load_dotenv()\n",
    "USERNAME = os.getenv('USERNAME')\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "\n",
    "# Countries\n",
    "COUNTRIES = [\n",
    "    # \"singapore\",\n",
    "    # \"laos\",\n",
    "    # \"malaysia\",\n",
    "    # \"cambodia\",\n",
    "    # \"indonesia\",\n",
    "    # \"hong kong\",\n",
    "    \"philippines\",\n",
    "    \"taiwan\",\n",
    "    \"vietnam\",\n",
    "    \"israel\",\n",
    "    \"india\",\n",
    "    \"thailand\",\n",
    "    \"korea\",\n",
    "    \"japan\",\n",
    "    \"china\",\n",
    "    \"new zealand\",\n",
    "    \"australia\"\n",
    "]\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "DIRECTORY = \"../Results/Untappd Search\"\n",
    "\n",
    "def last_csv(countries):\n",
    "    csv_files = []\n",
    "    \n",
    "    for country in countries:\n",
    "        # Construct the full file name with .csv extension\n",
    "        file_name = f\"{country}.csv\"\n",
    "        file_path = os.path.join(DIRECTORY, file_name)\n",
    "        \n",
    "        # Check if the file exists in the directory\n",
    "        if os.path.exists(file_path):\n",
    "            csv_files.append(file_path)  # Store the full path instead of just the file name\n",
    "    \n",
    "    # Return the path of the last CSV file found, or None if no files were found\n",
    "    return csv_files[-1] if csv_files else None\n",
    "\n",
    "def last_record(file_path):\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the DataFrame is not empty\n",
    "            if not df.empty:\n",
    "                # Return the last row of the DataFrame\n",
    "                return df.iloc[-1]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "def login():\n",
    "    # MANUAL STEPS\n",
    "    # ============\n",
    "    # 1. Verify the CAPTCHA\n",
    "    # 2. Press the Enter key to continue after verifying the CAPTCHA\n",
    "    # 3. Press \"Enter\" key in VSCode terminal to continue (ensures that login has been successful)\n",
    "\n",
    "    driver.get(LOGIN_URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Load cookies if they exist and check if already logged in\n",
    "    if os.path.exists(COOKIES_FILE_PATH):\n",
    "        load_cookies(driver)\n",
    "        driver.get(HOME_URL)  # Navigate to home to check if login was successful\n",
    "        if driver.current_url == HOME_URL:\n",
    "            print(\"Logged in using saved cookies\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Failed to log in with saved cookies. Proceeding with manual login.\")\n",
    "\n",
    "    # Enter username\n",
    "    username_input = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    username_input.send_keys(USERNAME)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "    password_input.send_keys(PASSWORD)\n",
    "\n",
    "    try:\n",
    "        # Wait for the CAPTCHA and solve it manually if it appears\n",
    "        captcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "        if captcha_frame:\n",
    "            print(\"Please solve the CAPTCHA manually if it appears, then press Enter to continue...\")\n",
    "            input()\n",
    "    except:\n",
    "        # No CAPTCHA present, continue with login\n",
    "        pass\n",
    "\n",
    "    # Wait until redirected to HOME_URL\n",
    "    wait.until(EC.url_to_be(HOME_URL))\n",
    "    print(\"Login successful, redirected to home\")\n",
    "\n",
    "    # Save cookies after successful login\n",
    "    save_cookies(driver)\n",
    "\n",
    "def get_soup():\n",
    "    time.sleep(2)  # Ensure the page has fully loaded\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def click_show_more(class_name):\n",
    "    time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    # Check if there is an \"announcement\" modal\n",
    "    try:\n",
    "        announcement_modal = driver.find_element(By.CLASS_NAME, 'announcement')\n",
    "        if announcement_modal:\n",
    "            track_button = announcement_modal.find_element(By.CLASS_NAME, 'track-click')\n",
    "            track_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "            # Find and click the \"Show More\" button\n",
    "            show_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, class_name))\n",
    "            )\n",
    "            ActionChains(driver).move_to_element(show_more_button).click().perform()\n",
    "            time.sleep(2)  # Ensure the new content has loaded\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "def parse_breweries(soup):\n",
    "    breweries = []\n",
    "    for brewery in soup.select('div.beer-details'):\n",
    "        name = brewery.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + brewery.select_one('p.name a')['href']\n",
    "        breweries.append((name, link))\n",
    "    print(f\"Found {len(breweries)} breweries\")\n",
    "    return breweries\n",
    "\n",
    "def parse_breweries_details(soup):\n",
    "    country = soup.select_one('p.brewery').text.strip() if soup.select_one('p.brewery') else \"\"\n",
    "    return country\n",
    "\n",
    "def parse_beers(soup):\n",
    "    beers = []\n",
    "    for beer in soup.select('div.beer-details'):\n",
    "        name = beer.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + beer.select_one('p.name a')['href']\n",
    "        beers.append((name, link))\n",
    "    print(f\"Found {len(beers)} beers\")\n",
    "    return beers\n",
    "\n",
    "def parse_beer_details(soup):\n",
    "    abv = soup.select_one('p.abv').text.strip() if soup.select_one('p.abv') else \"\"\n",
    "    drink_category = soup.select_one('div.name p.style').text.strip() if soup.select_one('div.name p.style') else \"\"\n",
    "    latest_review_date = soup.select_one('div.bottom a.time')['data-gregtime'] if soup.select_one('div.bottom a.time') else \"\"\n",
    "    official_description = soup.select_one('div.beer-descrption-read-less').text.strip() if soup.select_one('div.beer-descrption-read-less') else \"\"\n",
    "    if official_description.endswith(\" Show Less\"):\n",
    "        official_description = official_description[:-10]  # Remove the last 10 characters (\" Show Less\")\n",
    "    photo_link = soup.select_one('a.label.image-big')['data-image'] if soup.select_one('a.label.image-big') else \"\"\n",
    "    return abv, drink_category, latest_review_date, official_description, photo_link\n",
    "\n",
    "def scrape_untappd():\n",
    "    login()  # Perform login\n",
    "\n",
    "    # Extract last scraped details\n",
    "    last_csv_file = last_csv(COUNTRIES)  # Get the last CSV file path\n",
    "    last_row = last_record(last_csv_file)  # Get the last row from the last CSV file\n",
    "\n",
    "    start_brewery_idx = 0\n",
    "    start_beer_idx = 0\n",
    "    start_country_idx = 0\n",
    "\n",
    "    if last_csv_file is not None and last_row is not None:\n",
    "        # Continue from last scraped brewery and beer\n",
    "        start_brewery_idx = last_row['Brewery Index'] - 1\n",
    "        start_beer_idx = last_row['Beer Index']\n",
    "        country = last_csv_file.split('/')[-1].split('.')[0]\n",
    "        # Find the index of the country to start from\n",
    "        for idx, country in enumerate(COUNTRIES):\n",
    "            if country in last_csv_file:\n",
    "                start_country_idx = idx\n",
    "                break\n",
    "\n",
    "    for country_idx, search_country in enumerate(COUNTRIES[start_country_idx:], start=start_country_idx):\n",
    "        # Check if \"search_country\" contains \" \", replace with \"+\" if it does\n",
    "        search_country_formatted = search_country\n",
    "        if \" \" in search_country:\n",
    "            search_country_formatted = search_country.replace(\" \", \"+\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Scraping country: {search_country}\")\n",
    "\n",
    "        SEARCH_URL = f\"{BASE_URL}/search?q={search_country_formatted}&type=brewery&sort=\"\n",
    "        driver.get(SEARCH_URL)\n",
    "        click_show_more('more_search')  # Ensure all breweries are loaded\n",
    "        soup = get_soup()\n",
    "        breweries = parse_breweries(soup)\n",
    "\n",
    "        # ===== TO DELETE =====\n",
    "        breweries = breweries[:1]  # Limit to 1 brewery for testing\n",
    "        # ====================\n",
    "\n",
    "        beer_data = []\n",
    "\n",
    "        for brewery_idx, (brewery_name, brewery_url) in enumerate(breweries):\n",
    "            if country_idx == start_country_idx and brewery_idx < start_brewery_idx:\n",
    "                continue\n",
    "\n",
    "            print(\"==================================================\")\n",
    "            print(f\"Scraping brewery {brewery_idx + 1}/{len(breweries)}: {brewery_name}\")\n",
    "\n",
    "            # Step B: Scrape beers from each brewery\n",
    "            driver.get(f\"{brewery_url}/beer\")\n",
    "            click_show_more('more-list-items')  # Ensure all beers are loaded\n",
    "            soup = get_soup()\n",
    "            beers = parse_beers(soup)\n",
    "\n",
    "            # ===== TO DELETE =====\n",
    "            beers = beers[:1]  # Limit to 1 brewery for testing\n",
    "            # ====================\n",
    "\n",
    "            # Step C: Scrape brewery details\n",
    "            country = parse_breweries_details(soup)\n",
    "            for beer_idx, (beer_name, beer_url) in enumerate(beers):\n",
    "                if country_idx == start_country_idx and brewery_idx == start_brewery_idx and beer_idx < start_beer_idx:\n",
    "                    continue\n",
    "\n",
    "                print(f\"Scraping beer {beer_idx + 1}/{len(beers)}: {beer_name}\")\n",
    "\n",
    "                # Step D: Scrape beer details\n",
    "                driver.get(beer_url)\n",
    "                beer_soup = get_soup()\n",
    "                abv, drink_category, latest_review_date, official_description, photo_link = parse_beer_details(beer_soup)\n",
    "\n",
    "                # Collect beer data\n",
    "                beer_data = {\n",
    "                    'Brewery Index': brewery_idx + 1,\n",
    "                    'Beer Index': beer_idx + 1,\n",
    "                    'Scraping Remarks / Errors': '',  # Add any remarks if needed\n",
    "                    'Date of latest review': latest_review_date,\n",
    "                    'Expression Name': beer_name,\n",
    "                    'Producer': brewery_name,\n",
    "                    'Bottler': '', # Leave blank for now\n",
    "                    'Country of Origin': country,\n",
    "                    'Drink Type': 'Beer',\n",
    "                    'Drink Category': drink_category,\n",
    "                    'Age': '',\n",
    "                    'ABV': abv,\n",
    "                    '88B Website Review Link': '', # Leave blank for now\n",
    "                    'Official Description': official_description,\n",
    "                    'Producer Website Link': '',\n",
    "                    'Photo Link': photo_link\n",
    "                }\n",
    "\n",
    "                # Step E: Save the collected data to CSV\n",
    "                # Save for each beer scraped\n",
    "\n",
    "                csv_file_path = f'{DIRECTORY}/{search_country}.csv'\n",
    "\n",
    "                # Check if the CSV file exists\n",
    "                if os.path.exists(csv_file_path):\n",
    "                    # Load the existing CSV file into a DataFrame\n",
    "                    df = pd.read_csv(csv_file_path)\n",
    "                    # Append the new data to the DataFrame\n",
    "                    df = pd.concat([df, pd.DataFrame([beer_data])], ignore_index=True)\n",
    "                else:\n",
    "                    # Create a new DataFrame if the CSV file does not exist\n",
    "                    df = pd.DataFrame([beer_data])\n",
    "\n",
    "                # Save the DataFrame to a CSV file\n",
    "                df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_untappd()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapper 2: Scrape Top Rated Results\n",
    "Scrape from `https://untappd.com/brewery/top_rated?country={country}&brewery_type={brewery_type}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in using saved cookies\n",
      "----------------------------------------\n",
      "Scraping country: united states\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Scraping brewery type: macro_brewery\n",
      "Found 50 breweries\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Scraping brewery type: micro_brewery\n",
      "Found 50 breweries\n",
      "==================================================\n",
      "Scraping brewery 1: Brujos Brewing\n",
      "Found 50 beers\n",
      "Scraping beer 7/50: TDH Mulciber (2024)\n",
      "Scraping beer 8/50: Magister Ignis (2023)\n",
      "Scraping beer 9/50: Lord of the Scorched Church (2023)\n",
      "Scraping beer 10/50: Sitra Achra (2022)\n",
      "Scraping beer 11/50: Thou (2024)\n",
      "Scraping beer 12/50: Sixes\n",
      "Scraping beer 13/50: Loneliness (2024)\n",
      "Scraping beer 14/50: San Nosferatu (2023)\n",
      "Scraping beer 15/50: Mulciber (2023)\n",
      "Scraping beer 16/50: Signum Flavum\n",
      "Scraping beer 17/50: El Loro de Oro\n",
      "Scraping beer 18/50: The Pact\n",
      "Scraping beer 19/50: Secular (2023)\n",
      "Scraping beer 20/50: Cuatro Jinetes: Nelson\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b_/6l6rrpzs5yqd2b5_sl147sq40000gn/T/ipykernel_80764/2101879992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;31m# Run the scraper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m \u001b[0mscrape_untappd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;31m# Close the driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b_/6l6rrpzs5yqd2b5_sl147sq40000gn/T/ipykernel_80764/2101879992.py\u001b[0m in \u001b[0;36mscrape_untappd\u001b[0;34m()\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;31m# Step D: Scrape beer details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0mbeer_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0mabv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrink_category\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_review_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mofficial_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_beer_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;34m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mtrimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trim_large_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrimmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             return self.request_encode_body(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    716\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "# ============\n",
    "# === BEAUTIFULSOUP ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === DOTENV ===\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === OS ===\n",
    "import os\n",
    "\n",
    "# === PANDAS ===\n",
    "import pandas as pd\n",
    "\n",
    "# === PICKLE ===\n",
    "import pickle\n",
    "\n",
    "# === SELENIUM ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === TIME ===\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# SAVE LOGIN DETAILS\n",
    "# ============\n",
    "\n",
    "COOKIES_FILE_PATH = 'cookies.pkl'\n",
    "\n",
    "def save_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'rb') as file:\n",
    "        cookies = pickle.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://untappd.com\"\n",
    "LOGIN_URL = f\"{BASE_URL}/login\"\n",
    "HOME_URL = f\"{BASE_URL}/home\"\n",
    "\n",
    "# Credentials\n",
    "load_dotenv()\n",
    "USERNAME = os.getenv('USERNAME')\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "\n",
    "# Countries\n",
    "COUNTRIES = [\n",
    "    \"united states\",\n",
    "    \"canada\",\n",
    "    \"england\",\n",
    "    \"scotland\",\n",
    "    \"ireland\",\n",
    "    \"northern ireland\",\n",
    "    # \"wales\",\n",
    "    # \"germany\",\n",
    "    # \"belgium\",\n",
    "    # \"austria\",\n",
    "    # \"netherlands\",\n",
    "    # \"poland\",\n",
    "    # \"czech republic\",\n",
    "    # \"norway\",\n",
    "    # \"france\",\n",
    "    # \"denmark\",\n",
    "    # \"sweden\"\n",
    "]\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "DIRECTORY = \"../Results/Untappd Top Rated\"\n",
    "\n",
    "def last_csv(countries):\n",
    "    csv_files = []\n",
    "    \n",
    "    for country in countries:\n",
    "        # Construct the full file name with .csv extension\n",
    "        file_name = f\"{country}.csv\"\n",
    "        file_path = os.path.join(DIRECTORY, file_name)\n",
    "        \n",
    "        # Check if the file exists in the directory\n",
    "        if os.path.exists(file_path):\n",
    "            csv_files.append(file_path)  # Store the full path instead of just the file name\n",
    "    \n",
    "    # Return the path of the last CSV file found, or None if no files were found\n",
    "    return csv_files[-1] if csv_files else None\n",
    "\n",
    "def last_record(file_path):\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the DataFrame is not empty\n",
    "            if not df.empty:\n",
    "                # Return the last row of the DataFrame\n",
    "                return df.iloc[-1]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "def login():\n",
    "    # MANUAL STEPS\n",
    "    # ============\n",
    "    # 1. Verify the CAPTCHA\n",
    "    # 2. Press the Enter key to continue after verifying the CAPTCHA\n",
    "    # 3. Press \"Enter\" key in VSCode terminal to continue (ensures that login has been successful)\n",
    "\n",
    "    driver.get(LOGIN_URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Load cookies if they exist and check if already logged in\n",
    "    if os.path.exists(COOKIES_FILE_PATH):\n",
    "        load_cookies(driver)\n",
    "        driver.get(HOME_URL)  # Navigate to home to check if login was successful\n",
    "        if driver.current_url == HOME_URL:\n",
    "            print(\"Logged in using saved cookies\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Failed to log in with saved cookies. Proceeding with manual login.\")\n",
    "\n",
    "    # Enter username\n",
    "    username_input = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    username_input.send_keys(USERNAME)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "    password_input.send_keys(PASSWORD)\n",
    "\n",
    "    try:\n",
    "        # Wait for the CAPTCHA and solve it manually if it appears\n",
    "        captcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "        if captcha_frame:\n",
    "            print(\"Please solve the CAPTCHA manually if it appears, then press Enter to continue...\")\n",
    "            input()\n",
    "    except:\n",
    "        # No CAPTCHA present, continue with login\n",
    "        pass\n",
    "\n",
    "    # Wait until redirected to HOME_URL\n",
    "    wait.until(EC.url_to_be(HOME_URL))\n",
    "    print(\"Login successful, redirected to home\")\n",
    "\n",
    "    # Save cookies after successful login\n",
    "    save_cookies(driver)\n",
    "\n",
    "def get_soup():\n",
    "    time.sleep(2)  # Ensure the page has fully loaded\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def click_show_more(class_name):\n",
    "    time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    # Check if there is an \"announcement\" modal\n",
    "    try:\n",
    "        announcement_modal = driver.find_element(By.CLASS_NAME, 'announcement')\n",
    "        if announcement_modal:\n",
    "            track_button = announcement_modal.find_element(By.CLASS_NAME, 'track-click')\n",
    "            track_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "            # Find and click the \"Show More\" button\n",
    "            show_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, class_name))\n",
    "            )\n",
    "            ActionChains(driver).move_to_element(show_more_button).click().perform()\n",
    "            time.sleep(2)  # Ensure the new content has loaded\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "def parse_breweries(soup):\n",
    "    breweries = []\n",
    "    for brewery in soup.select('div.beer-details'):\n",
    "        name = brewery.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + brewery.select_one('p.name a')['href']\n",
    "        breweries.append((name, link))\n",
    "    print(f\"Found {len(breweries)} breweries\")\n",
    "    return breweries\n",
    "\n",
    "def parse_breweries_details(soup):\n",
    "    country = soup.select_one('p.brewery').text.strip() if soup.select_one('p.brewery') else \"\"\n",
    "    return country\n",
    "\n",
    "def parse_beers(soup):\n",
    "    beers = []\n",
    "    for beer in soup.select('div.beer-details'):\n",
    "        name = beer.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + beer.select_one('p.name a')['href']\n",
    "        beers.append((name, link))\n",
    "    print(f\"Found {len(beers)} beers\")\n",
    "    return beers\n",
    "\n",
    "def parse_beer_details(soup):\n",
    "    abv = soup.select_one('p.abv').text.strip() if soup.select_one('p.abv') else \"\"\n",
    "    drink_category = soup.select_one('div.name p.style').text.strip() if soup.select_one('div.name p.style') else \"\"\n",
    "    latest_review_date = soup.select_one('div.bottom a.time')['data-gregtime'] if soup.select_one('div.bottom a.time') else \"\"\n",
    "    official_description = soup.select_one('div.beer-descrption-read-less').text.strip() if soup.select_one('div.beer-descrption-read-less') else \"\"\n",
    "    if official_description.endswith(\" Show Less\"):\n",
    "        official_description = official_description[:-10]  # Remove the last 10 characters (\" Show Less\")\n",
    "    photo_link = soup.select_one('a.label.image-big')['data-image'] if soup.select_one('a.label.image-big') else \"\"\n",
    "    return abv, drink_category, latest_review_date, official_description, photo_link\n",
    "\n",
    "def scrape_untappd():\n",
    "    login()  # Perform login\n",
    "\n",
    "    # Get all brewery types except \"all\"\n",
    "    driver.get(f\"{BASE_URL}/brewery/top_rated?country=&brewery_type=\")\n",
    "    soup = get_soup()\n",
    "    brewery_types = [option['data-value-slug'] for option in soup.select('select#filter_picker option') if option['value'] != 'all']\n",
    "\n",
    "    # Extract last scraped details\n",
    "    last_csv_file = last_csv(COUNTRIES)  # Get the last CSV file path\n",
    "    last_row = last_record(last_csv_file)  # Get the last row from the last CSV file\n",
    "\n",
    "    start_brewery_idx = 0\n",
    "    start_beer_idx = 0\n",
    "    start_country_idx = 0\n",
    "\n",
    "    if last_csv_file is not None and last_row is not None:\n",
    "        # Continue from last scraped brewery and beer\n",
    "        start_brewery_idx = last_row['Brewery Index'] - 1\n",
    "        start_beer_idx = last_row['Beer Index']\n",
    "        country = last_csv_file.split('/')[-1].split('.')[0]\n",
    "        # Find the index of the country to start from\n",
    "        for idx, country in enumerate(COUNTRIES):\n",
    "            if country in last_csv_file:\n",
    "                start_country_idx = idx\n",
    "                break\n",
    "\n",
    "    # Initialize global brewery index counter\n",
    "    global_brewery_idx = 0\n",
    "\n",
    "    for country_idx, search_country in enumerate(COUNTRIES[start_country_idx:], start=start_country_idx):\n",
    "        # Check if \"search_country\" contains \" \", replace with \"-\" if it does\n",
    "        search_country_formatted = search_country\n",
    "        if \" \" in search_country:\n",
    "            search_country_formatted = search_country.replace(\" \", \"-\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Scraping country: {search_country}\")\n",
    "\n",
    "        for brewery_type in brewery_types:\n",
    "            print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
    "            print(f\"Scraping brewery type: {brewery_type}\")\n",
    "            SEARCH_URL = f\"{BASE_URL}/brewery/top_rated?country={search_country_formatted}&brewery_type={brewery_type}\"\n",
    "            driver.get(SEARCH_URL)\n",
    "            time.sleep(2)  # Wait for the page to load\n",
    "            soup = get_soup()\n",
    "            breweries = parse_breweries(soup)\n",
    "\n",
    "            # Determine if we need to skip this type based on the global index\n",
    "            if global_brewery_idx + len(breweries) <= start_brewery_idx:\n",
    "                global_brewery_idx += len(breweries)\n",
    "                continue\n",
    "\n",
    "            beer_data = []\n",
    "\n",
    "            for brewery_idx, (brewery_name, brewery_url) in enumerate(breweries):\n",
    "                if global_brewery_idx < start_brewery_idx:\n",
    "                    global_brewery_idx += 1\n",
    "                    continue\n",
    "\n",
    "                print(\"==================================================\")\n",
    "                print(f\"Scraping brewery {brewery_idx + 1}: {brewery_name}\")\n",
    "\n",
    "                # Step B: Scrape beers from each brewery\n",
    "                driver.get(f\"{brewery_url}/beer\")\n",
    "\n",
    "                click_show_more('more-list-items')  # Ensure all beers are loaded\n",
    "\n",
    "                soup = get_soup()\n",
    "                beers = parse_beers(soup)\n",
    "\n",
    "                # Step C: Scrape brewery details\n",
    "                country = parse_breweries_details(soup)\n",
    "                for beer_idx, (beer_name, beer_url) in enumerate(beers):\n",
    "                    if global_brewery_idx == start_brewery_idx and beer_idx < start_beer_idx:\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Scraping beer {beer_idx + 1}/{len(beers)}: {beer_name}\")\n",
    "\n",
    "                    # Step D: Scrape beer details\n",
    "                    driver.get(beer_url)\n",
    "                    beer_soup = get_soup()\n",
    "                    abv, drink_category, latest_review_date, official_description, photo_link = parse_beer_details(beer_soup)\n",
    "\n",
    "                    # Collect beer data\n",
    "                    beer_data = {\n",
    "                        'Brewery Index': global_brewery_idx + 1,\n",
    "                        'Beer Index': beer_idx + 1,\n",
    "                        'Scraping Remarks / Errors': '',  # Add any remarks if needed\n",
    "                        'Date of latest review': latest_review_date,\n",
    "                        'Expression Name': beer_name,\n",
    "                        'Producer': brewery_name,\n",
    "                        'Bottler': '', # Leave blank for now\n",
    "                        'Country of Origin': country,\n",
    "                        'Drink Type': 'Beer',\n",
    "                        'Drink Category': drink_category,\n",
    "                        'Age': '',\n",
    "                        'ABV': abv,\n",
    "                        '88B Website Review Link': '', # Leave blank for now\n",
    "                        'Official Description': official_description,\n",
    "                        'Producer Website Link': '',\n",
    "                        'Photo Link': photo_link\n",
    "                    }\n",
    "\n",
    "                    # Step E: Save the collected data to CSV\n",
    "                    # Save for each beer scraped\n",
    "\n",
    "                    csv_file_path = f'{DIRECTORY}/{search_country}.csv'\n",
    "\n",
    "                    # Check if the CSV file exists\n",
    "                    if os.path.exists(csv_file_path):\n",
    "                        # Load the existing CSV file into a DataFrame\n",
    "                        df = pd.read_csv(csv_file_path)\n",
    "                        # Append the new data to the DataFrame\n",
    "                        df = pd.concat([df, pd.DataFrame([beer_data])], ignore_index=True)\n",
    "                    else:\n",
    "                        # Create a new DataFrame if the CSV file does not exist\n",
    "                        df = pd.DataFrame([beer_data])\n",
    "\n",
    "                    # Save the DataFrame to a CSV file\n",
    "                    df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "                # Increment the global brewery index\n",
    "                global_brewery_idx += 1\n",
    "\n",
    "# Run the scraper\n",
    "scrape_untappd()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
