{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BEAUTIFULSOUP ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === OS ===\n",
    "import os\n",
    "\n",
    "# === PANDAS ===\n",
    "import pandas as pd\n",
    "\n",
    "# === PICKLE ===\n",
    "import pickle\n",
    "\n",
    "# === SELENIUM ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === TIME ===\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Login Details\n",
    "Function that saves login details as cookie information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOKIES_FILE_PATH = 'cookies.pkl'\n",
    "\n",
    "def save_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookies(driver, path=COOKIES_FILE_PATH):\n",
    "    with open(path, 'rb') as file:\n",
    "        cookies = pickle.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Webscrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL STEPS\n",
    "# ============\n",
    "# 1. Verify the CAPTCHA\n",
    "# 2. Press the Enter key to continue after verifying the CAPTCHA\n",
    "# 3. Press \"Enter\" key in VSCode terminal to continue (ensures that login has been successful)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://untappd.com\"\n",
    "LOGIN_URL = f\"{BASE_URL}/login\"\n",
    "HOME_URL = f\"{BASE_URL}/home\"\n",
    "SEARCH_URL = f\"{BASE_URL}/search?q=singapore&type=brewery&sort=\"\n",
    "\n",
    "# Credentials\n",
    "USERNAME = \"\"\n",
    "PASSWORD = \"\"\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "def login():\n",
    "    driver.get(LOGIN_URL)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Load cookies if they exist and check if already logged in\n",
    "    if os.path.exists(COOKIES_FILE_PATH):\n",
    "        load_cookies(driver)\n",
    "        driver.get(HOME_URL)  # Navigate to home to check if login was successful\n",
    "        if driver.current_url == HOME_URL:\n",
    "            print(\"Logged in using saved cookies\")\n",
    "            driver.get(SEARCH_URL)\n",
    "            return\n",
    "        else:\n",
    "            print(\"Failed to log in with saved cookies. Proceeding with manual login.\")\n",
    "\n",
    "    # Enter username\n",
    "    username_input = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    username_input.send_keys(USERNAME)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "    password_input.send_keys(PASSWORD)\n",
    "\n",
    "    try:\n",
    "        # Wait for the CAPTCHA and solve it manually if it appears\n",
    "        captcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "        if captcha_frame:\n",
    "            print(\"Please solve the CAPTCHA manually if it appears, then press Enter to continue...\")\n",
    "            input()\n",
    "    except:\n",
    "        # No CAPTCHA present, continue with login\n",
    "        pass\n",
    "\n",
    "    # Wait until redirected to HOME_URL\n",
    "    wait.until(EC.url_to_be(HOME_URL))\n",
    "    print(\"Login successful, redirected to home\")\n",
    "\n",
    "    # Save cookies after successful login\n",
    "    save_cookies(driver)\n",
    "\n",
    "    # Redirect to SEARCH_URL\n",
    "    driver.get(SEARCH_URL)\n",
    "    print(\"Navigated to search page\")\n",
    "\n",
    "\n",
    "def get_soup():\n",
    "    time.sleep(2)  # Ensure the page has fully loaded\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def click_show_more(class_name):\n",
    "    time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    # Check if there is an \"announcement\" modal\n",
    "    try:\n",
    "        announcement_modal = driver.find_element(By.CLASS_NAME, 'announcement')\n",
    "        if announcement_modal:\n",
    "            track_button = announcement_modal.find_element(By.CLASS_NAME, 'track-click')\n",
    "            track_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "            # Find and click the \"Show More\" button\n",
    "            show_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, class_name))\n",
    "            )\n",
    "            ActionChains(driver).move_to_element(show_more_button).click().perform()\n",
    "            time.sleep(2)  # Ensure the new content has loaded\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "def parse_breweries(soup):\n",
    "    breweries = []\n",
    "    for brewery in soup.select('div.beer-details'):\n",
    "        name = brewery.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + brewery.select_one('p.name a')['href']\n",
    "        breweries.append((name, link))\n",
    "    print(f\"Found {len(breweries)} breweries\")\n",
    "    return breweries\n",
    "\n",
    "def parse_breweries_details(soup):\n",
    "    country = soup.select_one('p.brewery').text.strip() if soup.select_one('p.brewery') else \"\"\n",
    "    return country\n",
    "\n",
    "def parse_beers(soup):\n",
    "    beers = []\n",
    "    for beer in soup.select('div.beer-details'):\n",
    "        name = beer.select_one('p.name a').text.strip()\n",
    "        link = BASE_URL + beer.select_one('p.name a')['href']\n",
    "        beers.append((name, link))\n",
    "    print(f\"Found {len(beers)} beers\")\n",
    "    return beers\n",
    "\n",
    "def parse_beer_details(soup):\n",
    "    abv = soup.select_one('p.abv').text.strip() if soup.select_one('p.abv') else \"\"\n",
    "    drink_category = soup.select_one('div.name p.style').text.strip() if soup.select_one('div.name p.style') else \"\"\n",
    "    latest_review_date = soup.select_one('div.bottom a.time')['data-gregtime'] if soup.select_one('div.bottom a.time') else \"\"\n",
    "    official_description = soup.select_one('div.beer-descrption-read-less').text.strip() if soup.select_one('div.beer-descrption-read-less') else \"\"\n",
    "    if official_description.endswith(\" Show Less\"):\n",
    "        official_description = official_description[:-10]  # Remove the last 10 characters (\" Show Less\")\n",
    "    photo_link = soup.select_one('a.label.image-big')['data-image'] if soup.select_one('a.label.image-big') else \"\"\n",
    "    return abv, drink_category, latest_review_date, official_description, photo_link\n",
    "\n",
    "def scrape_untappd():\n",
    "    login()  # Perform login\n",
    "\n",
    "    beer_data = []\n",
    "\n",
    "    # Step A: Scrape all breweries\n",
    "    click_show_more('more_search')  # Ensure all breweries are loaded\n",
    "    soup = get_soup()\n",
    "    breweries = parse_breweries(soup)\n",
    "\n",
    "    for brewery_idx, (brewery_name, brewery_url) in enumerate(breweries):\n",
    "        print(\"==================================================\")\n",
    "        print(f\"Scraping brewery {brewery_idx + 1}/{len(breweries)}: {brewery_name}\")\n",
    "\n",
    "        # Step B: Scrape beers from each brewery\n",
    "        driver.get(f\"{brewery_url}/beer\")\n",
    "        click_show_more('more-list-items ')  # Ensure all beers are loaded\n",
    "        soup = get_soup()\n",
    "        beers = parse_beers(soup)\n",
    "\n",
    "        # Step C: Scrape brewery details\n",
    "        country = parse_breweries_details(soup)\n",
    "        for beer_idx, (beer_name, beer_url) in enumerate(beers):\n",
    "            print(f\"Scraping beer {beer_idx + 1}/{len(beers)}: {beer_name}\")\n",
    "\n",
    "            # Step D: Scrape beer details\n",
    "            driver.get(beer_url)\n",
    "            beer_soup = get_soup()\n",
    "            abv, drink_category, latest_review_date, official_description, photo_link = parse_beer_details(beer_soup)\n",
    "\n",
    "            # Collect beer data\n",
    "            beer_data.append({\n",
    "                'Brewery Index': brewery_idx + 1,\n",
    "                'Beer Index': beer_idx + 1,\n",
    "                'Scraping Remarks / Errors': '',  # Add any remarks if needed\n",
    "                'Date of latest review': latest_review_date,\n",
    "                'Expression Name': beer_name,\n",
    "                'Producer': brewery_name,\n",
    "                'Bottler': '', # Leave blank for now\n",
    "                'Country of Origin': country,\n",
    "                'Drink Type': 'Beer',\n",
    "                'Drink Category': drink_category,\n",
    "                'Age': '',\n",
    "                'ABV': abv,\n",
    "                '88B Website Review Link': '', # Leave blank for now\n",
    "                'Official Description': official_description,\n",
    "                'Producer Website Link': '',\n",
    "                'Photo Link': photo_link\n",
    "            })\n",
    "\n",
    "        # Step E: Save the collected data to CSV\n",
    "        # Save for every producer scraped\n",
    "        df = pd.DataFrame(beer_data)\n",
    "        df.to_csv('untappd_beers.csv', index=False)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_untappd()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
